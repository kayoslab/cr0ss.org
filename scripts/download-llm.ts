/**
 * Pre-download LLM models at build time
 * This ensures the model is cached before the app runs
 */

import { pipeline, env } from "@huggingface/transformers";
import { AVAILABLE_MODELS, MODELS_TO_PRELOAD } from "../lib/ai/models";

// Configure cache directory
env.cacheDir = "./.transformers-cache";

async function downloadModels() {
  console.log("ðŸ¤– Pre-downloading LLM models...\n");

  for (const modelKey of MODELS_TO_PRELOAD) {
    const model = AVAILABLE_MODELS[modelKey as keyof typeof AVAILABLE_MODELS];
    if (!model) {
      console.warn(`âš ï¸  Model ${modelKey} not found in AVAILABLE_MODELS`);
      continue;
    }

    console.log(`ðŸ“¥ Downloading: ${model.name} (${model.id})`);
    console.log(`   Size: ${model.size}`);

    try {
      // Download by initializing the pipeline
      // The model will be cached for subsequent use
      const pipe = await pipeline("text-generation", model.id);

      // Clean up
      if ("dispose" in pipe) await (pipe as { dispose: () => Promise<void> }).dispose();

      console.log(`âœ… ${model.name} downloaded and cached\n`);
    } catch (error) {
      console.error(`âŒ Failed to download ${model.name}:`, error);
      process.exit(1);
    }
  }

  console.log("ðŸŽ‰ All models downloaded successfully!");
}

downloadModels().catch((error) => {
  console.error("Fatal error:", error);
  process.exit(1);
});
